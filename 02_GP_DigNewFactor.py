import warnings
import os
from tqdm import tqdm
from gplearn.functions import make_function
from gplearn.genetic import SymbolicTransformer
from sklearn.utils.validation import check_X_y, check_array
import pandas as pd
import numpy as np
from gplearn.genetic import SymbolicTransformer

warnings.filterwarnings("ignore")

# ================= 0. å…¼å®¹æ€§ä¿®å¤ (Monkey Patch å¢å¼ºç‰ˆ) =================
# ä¿®å¤ scikit-learn >= 1.6 ä¸ gplearn çš„æ ¸å¿ƒå…¼å®¹æ€§é—®é¢˜
if not hasattr(SymbolicTransformer, '_validate_data'):
    def _validate_data(self, X, y=None, y_numeric=False, **kwargs):
        if y is not None:
            X, y = check_X_y(X, y, **kwargs)
        else:
            X = check_array(X, **kwargs)

        if not hasattr(self, 'n_features_in_'):
            if X.ndim == 2:
                self.n_features_in_ = X.shape[1]
            else:
                self.n_features_in_ = 1
        return X, y
    SymbolicTransformer._validate_data = _validate_data

#   é…ç½®é¡¹
Input_file='sz100.csv'
Output_file='sz100_GP_Factor.csv'
Target_period = 5  # é¢„æµ‹ 5 æ—¥åæ”¶ç›Š
GP_generation = 20  # è¿›åŒ–ä»£æ•°
GP_population = 3000  # ç§ç¾¤å¤§å°
GP_components = 5  # ä¿ç•™ Top 10 å› å­



# ================= 1. ä¼ ç»Ÿå› å­æ„å»º (æ‰©å……ç‰ˆ) =================
def add_traditional_factors(df):
    print("æ­£åœ¨æ„å»ºä¼ ç»Ÿå› å­åº“ (å«Kçº¿ç»“æ„ã€é‡ä»·ç›¸å…³æ€§)...")
    data = df.copy()
    # ä¿æŒcodeè¿™ä¸€åˆ—æ˜¯strç±»å‹çš„;
    data['code'] = data['code'].astype(str)

    # --- A. Momentum ---
    for t in [5, 10, 20, 60]:
        data[f'ROC_{t}'] = data.groupby('code')['close'].pct_change(periods=t)

    def calculate_rsi(series, period=14):
        delta = series.diff()
        gain = (delta.where(delta > 0, 0)).rolling(period).mean()
        loss = (-delta.where(delta < 0, 0)).rolling(period).mean()
        rs = gain / loss
        return 100 - (100 / (1 + rs))

    data['RSI_14'] = data.groupby('code')['close'].transform(lambda x: calculate_rsi(x, 14))

    # BIAS
    for t in [5, 20, 60]:
        ma = data.groupby('code')['close'].transform(lambda x: x.rolling(t).mean())
        data[f'BIAS_{t}'] = (data['close'] - ma) / ma

    # --- B. Volatility ---
    data['RET'] = data.groupby('code')['close'].pct_change()
    for t in [5, 20, 60]:
        data[f'STD_{t}'] = data.groupby('code')['RET'].transform(lambda x: x.rolling(t).std())

    # ATR (ç»å¯¹å€¼ï¼Œç”¨äºè®¡ç®— NATR)
    high_low = data['high'] - data['low']
    high_close = (data['high'] - data['close'].shift()).abs()
    low_close = (data['low'] - data['close'].shift()).abs()
    tr_series = pd.concat([high_low, high_close, low_close], axis=1).max(axis=1)

    data['ATR_14'] = tr_series.groupby(data['code']).transform(lambda x: x.rolling(14).mean())
    data['NATR_14'] = data['ATR_14'] / data['close']  # å½’ä¸€åŒ– ATR

    # --- C. Volume ---
    data['VOL_CHANGE'] = data.groupby('code')['volume'].pct_change()

    # VWAP (ç»å¯¹ä»·æ ¼ï¼Œåç»­å‰”é™¤)
    data['VWAP_D'] = data['amount'] / (data['volume'] * 100 + 1e-9)
    data['VWAP_REL'] = data['close'] / data['VWAP_D']  # å½’ä¸€åŒ– VWAP

    data['VOL_STD_20'] = data.groupby('code')['volume'].transform(lambda x: (x / x.rolling(20).mean()))

    # --- ã€æ–°å¢ã€‘ é‡ä»·ç›¸å…³æ€§ (CORR_PV) ---
    # é€»è¾‘ï¼šè®¡ç®—è¿‡å»10å¤©æ”¶ç›˜ä»·ä¸æˆäº¤é‡çš„ç›¸å…³ç³»æ•° (-1 åˆ° 1)
    # 1 è¡¨ç¤ºé‡ä»·é½å‡ï¼Œ-1 è¡¨ç¤ºé‡ä»·èƒŒç¦»
    # reset_index(level=0, drop=True) æ˜¯ä¸ºäº†å¯¹é½ groupby åçš„ç´¢å¼•
    print("  >> è®¡ç®—é‡ä»·ç›¸å…³æ€§...")
    data['CORR_PV_10'] = data.groupby('code').apply(
        lambda x: x['close'].rolling(10).corr(x['volume'])
    ).reset_index(level=0, drop=True)

    # --- D. Trend ---
    ema12 = data.groupby('code')['close'].transform(lambda x: x.ewm(span=12, adjust=False).mean())
    ema26 = data.groupby('code')['close'].transform(lambda x: x.ewm(span=26, adjust=False).mean())

    # âœ… ä¿®æ­£ç‰ˆ (å»é‡çº²ï¼Œé™¤ä»¥æ”¶ç›˜ä»·)
    data['MACD'] = (ema12 - ema26) / data['close']
    data['MACD_SIGNAL'] = data.groupby('code')['MACD'].transform(lambda x: x.ewm(span=9, adjust=False).mean())
    data['MACD_HIST'] = data['MACD'] - data['MACD_SIGNAL']

    # --- E. Reversal & Structure ---
    data['HL_PCT'] = (data['high'] - data['low']) / data['close']
    data['CO_PCT'] = (data['close'] - data['open']) / data['open']

    # --- ã€æ–°å¢ã€‘ Kçº¿å½¢æ€ç»“æ„ (Shadows & Body) ---
    # åˆ†æ¯ï¼šæ—¥å†…æŒ¯å¹… (åŠ å¾®å°å€¼é˜²æ­¢é™¤ä»¥0)
    range_hl = (data['high'] - data['low']).replace(0, np.nan).fillna(1e-9)

    # 1. ä¸Šå½±çº¿åŠ›åº¦: (High - Max(Open, Close)) / Range
    data['SHADOW_UP'] = (data['high'] - data[['open', 'close']].max(axis=1)) / range_hl

    # 2. ä¸‹å½±çº¿åŠ›åº¦: (Min(Open, Close) - Low) / Range
    data['SHADOW_DOWN'] = (data[['open', 'close']].min(axis=1) - data['low']) / range_hl

    # 3. å®ä½“åŠ›åº¦: Abs(Close - Open) / Range
    data['BODY_ABS'] = (data['close'] - data['open']).abs() / range_hl

    data.dropna(inplace=True)

    # ================= ğŸš¨ æ ¸å¿ƒä¿®æ­£æ­¥éª¤ ğŸš¨ =================
    # åœ¨è¿”å›æ•°æ®å‰ï¼Œå¿…é¡»å‰”é™¤æ‰é‚£äº›ä½œä¸ºä¸­é—´è®¡ç®—æ­¥éª¤çš„â€œç»å¯¹å€¼â€åˆ—
    # å¦åˆ™ GP ä¼šåˆ©ç”¨å®ƒä»¬è¿›è¡Œâ€œä½œå¼Šâ€ï¼ˆåˆ©ç”¨ä»·æ ¼ç»å¯¹å€¼å¤§å°é€‰è‚¡ï¼‰
    cols_to_drop = ['ATR_14', 'VWAP_D']

    # å®‰å…¨åˆ é™¤ï¼ˆæ£€æŸ¥åˆ—æ˜¯å¦å­˜åœ¨ï¼‰
    existing_cols_to_drop = [c for c in cols_to_drop if c in data.columns]
    if existing_cols_to_drop:
        data.drop(columns=existing_cols_to_drop, inplace=True)
        print(f"  >> å·²å‰”é™¤æœ‰é‡çº²å¹²æ‰°å› å­: {existing_cols_to_drop}")

    return data


# ================= ğŸš¨ æ ¸å¿ƒä¿®æ”¹: å®šä¹‰å¹¶æ³¨å†Œè‡ªå®šä¹‰ç®—å­ =================
def get_custom_functions():
    """
    å®šä¹‰ gplearn çš„è‡ªå®šä¹‰é€»è¾‘ç®—å­å’Œéçº¿æ€§ç®—å­
    """

    # 1. Signed Square: ä¿æŒç¬¦å·çš„å¹³æ–¹
    # é€»è¾‘: x * |x|ã€‚è¿™æ¯” x^2 å¥½ï¼Œå› ä¸ºå®ƒä¿ç•™äº†æ–¹å‘ï¼ˆè´Ÿæ”¶ç›Šå˜å¾—æ›´è´Ÿï¼Œæ­£æ”¶ç›Šå˜å¾—æ›´æ­£ï¼‰ã€‚
    def _signed_square(x):
        return np.sign(x) * (np.abs(x) ** 2)

    # make_function å°†æ™®é€š python å‡½æ•°è½¬æ¢ä¸º gplearn å¯ç”¨çš„ç®—å­
    # arity=1 è¡¨ç¤ºè¿™ä¸ªç®—å­æ¥å— 1 ä¸ªå‚æ•°
    signed_square = make_function(function=_signed_square, name='signed_square', arity=1)

    # 2. If_Else (Ternary Operator)
    # é€»è¾‘: å¦‚æœ Condition > 0ï¼Œåˆ™è¿”å› Aï¼Œå¦åˆ™è¿”å› B
    # ç±»ä¼¼äº pandas çš„ where æˆ– numpy çš„ where
    def _if_else(condition, true_val, false_val):
        return np.where(condition > 0, true_val, false_val)

    # arity=3 è¡¨ç¤ºè¿™ä¸ªç®—å­æ¥å— 3 ä¸ªå‚æ•° (Condition, A, B)
    if_else = make_function(function=_if_else, name='if_else', arity=3)

    return [signed_square, if_else]

def GP_Dig_Factor(df, feature_cols, target_cols):
    # å»ºè®®æŠŠè¿™é‡Œå¢åŠ åˆ° 50
    print(f"æ­£åœ¨æŒ–æ˜ GP Alpha å› å­ (Target={target_cols}, Gens={GP_generation})...")

    # 1. ä¸¥æ ¼æ—¶åºåˆ’åˆ† (ä¿æŒä½ åŸæ¥çš„ä¿®æ­£)
    unique_dates = df['date'].sort_values().unique()
    split_date = unique_dates[int(len(unique_dates) * 0.7)]
    print(f"ã€ä¸¥æ ¼æ—¶åºåˆ’åˆ†ã€‘è®­ç»ƒé›†æˆªæ­¢æ—¥æœŸ: {pd.to_datetime(split_date).strftime('%Y-%m-%d')}")

    train_df = df[df['date'] <= split_date].copy()

    # ========================== æ ¸å¿ƒä¿®æ”¹ A: è¾“å…¥ç‰¹å¾ ==========================
    # âŒ åŸä»£ç  (å·²æ³¨é‡Š): åªç”¨äº†åŸå§‹ä»·æ ¼ï¼Œé‡çº²æ··ä¹±
    # X_dict = {}
    # base_features = ['open', 'high', 'low', 'close', 'volume', 'amount']
    # ... (çœç•¥åŸä»£ç ) ...

    # âœ… æ–°ä»£ç : ä½¿ç”¨ä½ è®¡ç®—å¥½çš„ä¼ ç»Ÿå› å­ (RSI, ROC, BIAS ç­‰)
    # è¿™äº›å› å­å·²ç»æ˜¯æ¯”ç‡(Ratio)äº†ï¼Œéå¸¸é€‚åˆ GP ç»„åˆ
    # å¦‚æœä½ æƒ³ä¿ç•™åŸå§‹é‡ä»·çš„ lagï¼Œå¯ä»¥æŠŠå®ƒä»¬ä¹ŸåŠ è¿› feature_colsï¼Œä½†åœ¨ main å‡½æ•°é‡ŒåŠ 
    X = train_df[feature_cols].copy()
    # ç®€å•æ¸…æ´—ï¼šå¤„ç†ä¼ ç»Ÿå› å­å¯èƒ½äº§ç”Ÿçš„ inf æˆ– nan
    X = X.replace([np.inf, -np.inf], np.nan).fillna(0)
    # ========================================================================
    y = train_df[target_cols].fillna(0.5).values

    # ================= ğŸš¨ æ ¸å¿ƒä¿®æ”¹: å°†ç®—å­åŠ å…¥ GP =================
    # 1. è·å–è‡ªå®šä¹‰ç®—å­
    custom_funcs = get_custom_functions()

    # 2. å®šä¹‰åŸºç¡€ç®—å­ (ä¿ç•™ä½ åŸæ¥ç”¨çš„ä¸€äº›)
    base_functions = ['add', 'sub', 'mul', 'div', 'abs', 'neg', 'inv', 'max', 'min']

    # 3. åˆå¹¶æˆå®Œæ•´çš„ function_set
    function_set = base_functions + custom_funcs

    est = SymbolicTransformer(
        generations=40,  # âœ… å»ºè®®è°ƒå¤§åˆ° 50
        population_size=6000,  # âœ… ä¿æŒ 2000-3000
        hall_of_fame=100,
        n_components=GP_components,
        function_set=function_set,
        # ========================== æ ¸å¿ƒä¿®æ”¹ B: æƒ©ç½šç³»æ•° ==========================
        parsimony_coefficient=0.001,  # âœ… è°ƒå°100å€ï¼é¼“åŠ±å…¬å¼å˜é•¿ã€å˜å¤æ‚
        # ========================================================================
        max_samples=0.6,
        # 2. é™ä½ç«äº‰å‹åŠ› (è®©éä¸»æµå› å­ä¹Ÿèƒ½å­˜æ´»ï¼Œé˜²æ­¢åŒè´¨åŒ–)
        tournament_size=3,  # é»˜è®¤æ˜¯20ï¼Œè°ƒå°ä¸€ç‚¹å¢åŠ å¤šæ ·æ€§
        # 3. è°ƒæ•´è¿›åŒ–æ¦‚ç‡ (ç»„åˆæ‹³ï¼šå°‘æ‚äº¤ï¼Œå¤šçªå˜ï¼Œå¤šç²¾ç®€)
        p_crossover=0.1,  # é™ä½æ‚äº¤ (é˜²æ­¢è¿‘äº²ç¹æ®–)
        p_subtree_mutation=0.7,  # å¢åŠ æ–°é€»è¾‘æ³¨å…¥ (å¼•å…¥æ–°æ€è·¯)
        p_hoist_mutation=0.1,  # ã€æŠ—å¥—å¨ƒç¥å™¨ã€‘ä¸“é—¨å¯¹æŠ— sin(sin(...)) è¿™ç§ç»“æ„
        p_point_mutation=0.1,  # èŠ‚ç‚¹å¾®è°ƒ (æ¯”å¦‚æŠŠ max å˜æˆ min)

        verbose=1,
        random_state=42,
        n_jobs=-1  # âœ… å¼€å¯å¤šæ ¸å¹¶è¡Œï¼ŒåŠ é€Ÿè¿ç®—
    )

    print(f"å¼€å§‹è®­ç»ƒ (Feature Shape: {X.shape})...")
    est.fit(X, y)

    print("GP æŒ–æ˜å®Œæˆï¼ŒBest Alphas:")
    for i, expr in enumerate(est._best_programs):
        if i < GP_components:
            print(f"Alpha_{i}: {expr}")

    # åº”ç”¨åˆ°å…¨é‡æ•°æ® (åŒæ ·ä¿®æ­£ä¸ºä½¿ç”¨ feature_cols)
    full_X = df[feature_cols].copy()
    full_X = full_X.replace([np.inf, -np.inf], np.nan).fillna(0)

    new_features = est.transform(full_X)

    for i in range(new_features.shape[1]):
        df[f'GP_ALPHA_{i}'] = new_features[:, i]

    return df

# ================= 3. é¢„å¤„ç†å·¥ç¨‹ =================
def preprocess_factors(df, feature_cols):
    print("æ­£åœ¨è¿›è¡Œæˆªé¢é¢„å¤„ç† (Winsorize -> Z-Score)...")
    processed_df = df.copy()

    def process_day(group):
        cols = [c for c in group.columns if c in feature_cols]
        for col in cols:
            series = group[col]
            # Winsorizeï¼Œå»æ‰1%çš„æœ€å¤§å€¼å’Œ1%çš„æœ€å°å€¼
            lower = series.quantile(0.01)
            upper = series.quantile(0.99)
            series = series.clip(lower, upper)
            # Z-Scoreï¼Œæ ‡å‡†åŒ–
            std = series.std()
            if std != 0:
                series = (series - series.mean()) / std
            else:
                series = 0
            group[col] = series
        return group

    tqdm.pandas(desc="Cross-Section Scaling")
    processed_df = processed_df.groupby('date').progress_apply(process_day)

    return processed_df


# ================= ä¸»ç¨‹åº =================
def main():
    if not os.path.exists(Input_file):
        print(f"é”™è¯¯: æ‰¾ä¸åˆ° {Input_file}")
        return

    print(f"è¯»å–æ•°æ®: {Input_file}...")
    df = pd.read_csv(Input_file, dtype={'code': str})
    df['date'] = pd.to_datetime(df['date'])
    df = df.sort_values(['code', 'date']).reset_index(drop=True)

    # 1. è®¡ç®—åŸå§‹ 5æ—¥æ”¶ç›Šç‡
    df['RET_FWD_5'] = df.groupby('code')['close'].shift(-Target_period) / df['close'] - 1

    # ã€æ–°å¢é€»è¾‘ã€‘ 2. è®¡ç®— 5æ—¥æ”¶ç›Šç‡çš„æ’å (æˆªé¢æ’å)
    # pct=True è¡¨ç¤ºç™¾åˆ†æ¯”æ’å(0~1ä¹‹é—´)ï¼Œmethod='first' å¤„ç†å¹³å±€æƒ…å†µ
    # å¿…é¡»æŒ‰ date åˆ†ç»„ï¼Œå› ä¸ºæ’åæ˜¯æ¯å¤©æ‰€æœ‰è‚¡ç¥¨ä¹‹é—´çš„æ¯”è¾ƒ
    print("æ­£åœ¨è®¡ç®—æˆªé¢æ”¶ç›Šç‡æ’å (Rank)...")
    df['RET_FWD_5_RANK'] = df.groupby('date')['RET_FWD_5'].transform(lambda x: x.rank(pct=True, method='first'))

    # æ„å»ºä¼ ç»Ÿå› å­
    df = add_traditional_factors(df)

    # ç¡®å®šå“ªäº›åˆ—éœ€è¦ä¿ç•™ï¼Œå“ªäº›æ˜¯å› å­
    base_cols = ['date', 'code', 'name', 'open', 'high', 'low', 'close', 'volume', 'amount', 'RET_FWD_5',
                 'RET_FWD_5_RANK', 'RET']
    traditional_factors = [c for c in df.columns if c not in base_cols]
    print(f"å·²æ„å»º {len(traditional_factors)} ä¸ªä¼ ç»Ÿå› å­ã€‚")

    # å¯¹dfè¿›è¡Œé¢„å¤„ç†ï¼Œå»é‡
    df_clean = df.dropna().copy()

    print("ç‰¹å¾ç´¢å¼•å¯¹ç…§è¡¨:")
    for i, col in enumerate(traditional_factors):
        print(f"X{i}: {col}")

    # ã€é‡è¦é€‰æ‹©ã€‘
    # è¿™é‡Œçš„ target_col å¯ä»¥é€‰æ‹© 'RET_FWD_5' (é¢„æµ‹æ•°å€¼) ä¹Ÿå¯ä»¥é€‰æ‹© 'RET_FWD_5_RANK' (é¢„æµ‹æ’å)
    # å»ºè®®ä½¿ç”¨ RANKï¼Œå› ä¸ºå› å­çš„æ’åºèƒ½åŠ›é€šå¸¸æ¯”æ•°å€¼é¢„æµ‹èƒ½åŠ›æ›´é‡è¦
    # è¿™é‡Œæˆ‘æ”¹ä¸ºäº†ä¼ å…¥ 'RET_FWD_5_RANK'
    df_with_gp = GP_Dig_Factor(df_clean, traditional_factors, 'RET_FWD_5_RANK')

    # è¿™é‡Œè·å–gp_factors;å°±æ˜¯å¸¦æœ‰ALPHA
    gp_factors = [c for c in df_with_gp.columns if 'GP_ALPHA' in c]
    all_factors = traditional_factors + gp_factors
    print(f"å·²æŒ–æ˜ {len(gp_factors)} ä¸ª GP å› å­ã€‚")

    # é¢„å¤„ç†
    final_df = preprocess_factors(df_with_gp, all_factors)
    final_df.dropna(inplace=True)

    print(f"ä¿å­˜ç»“æœè‡³: {Output_file}")
    final_df.to_csv(Output_file, index=False)

    print("ç‰¹å¾å·¥ç¨‹å…¨éƒ¨å®Œæˆã€‚")
    print(f"æœ€ç»ˆç»´åº¦: {final_df.shape}")


if __name__ == "__main__":
    main()

