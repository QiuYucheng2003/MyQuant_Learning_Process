这个文件，我将从头到尾【getdata----backtrade】来完整实现我自己的量化开发项目;

1. 01_getdata.py这个文件获取的是上证50的数据，从2020-2024年，日线级别；
但是由于网络问题，没有获取成功，我直接用深圳100的股票数据，也是从2020-2024年的日线级别
数据存储在sz100.csv

【【【【  如何挖出好的因子，也是一个重大的方向——因子挖掘】】】】
——一个是模型训练的参数【这个需要自己决定】
——一个是训练的数据的算子，需要多样性【这个算子非常重要，需要用率，而不是数值】
——一个是label列的选择【我们一般都选择收益率排名rank】
2. 02_GP_DigNewFactor.py 这个文件用来挖掘新的因子，我们以5日收益率的排名Rank作为Label,去寻找与之最相关的因子
第二步中，挖到结果较好的因子，我才能进入第三步，就是因子评价以及进一步的因子优化（平滑处理）
结果：1.挖掘出来的因子不太成功，同质化严重；   2.量纲未处理，导致疯狂开根号sqrt
修改：1. 我们先进行量纲处理，因为量纲的未处理，导致挖掘出来的因子都是sqrt开根号，导致了因子同质化

修改1的运行结果：因子同质化解决，但同时有新的问题和新的解决方法：
1. 测试集在最后的几轮，IC值突然腰斩，是因为过拟合了，我们需要早停（early stop），因此要加入早停机制
2. 因子全是X5,X7的排列组合，还是存在同质化现象，写一个循环，挖出一个 Best Alpha 后，
把这个 Alpha 的预测结果从 y 中减去（正交化），让 GP 去挖掘 “剩下的、还没被解释的规律”。
3. 加入时序算子，真正的 Alpha 来源于 时序逻辑。 你需要加入 Ts_Rank (滚动排名), Ts_Corr (滚动相关性)。
在 build_gp_features 里直接算好喂给它。
X_dict['CORR_PV'] = 价格和成交量的10日相关系数；X_dict['RANK_RET'] = 今天的收益率在过去20天里的排名。

第二步GP因子挖掘的优化：
1. 确保喂给GP的特征矩阵都是要无量纲的，这样才能过滤掉股价的影响！【根本原因】
2. 优化GP挖掘时候的参数，确保惩罚系数已经迭代次数；

第二步的拓展：为了挖掘更高质量的因子，我需要加入更多特征
1. 换手率(turnover rate)
2. 上影线长度 (Upper Shadow): (High - max(Open, Close)) / (High - Low)
3. 下影线长度 (Lower Shadow): (min(Open, Close) - Low) / (High - Low)
4. K线实体占比 (Body Ratio): abs(Close - Open) / (High - Low)
5. 跳空幅度 (Gap): (Open - PreClose) / PreClose
6. 日内收益 (Intraday Ret): (Close - Open) / Open
7. CORR_PV_10、CORR_HL_10

在量化的因子挖掘中，除了量价挖掘，还可以加入其他维度的指标可以显著提升模型的解释力、降低过拟合风险并捕捉更丰富的市场规律。
1. 基本面与财务指标（市盈率、市净率等等）
2. 订单簿不平衡度：买卖盘口量价的比例关系，预测极短期价格压力。
3. 逐笔成交特征 & 流动性指标;


加了这些特征之后，继续训练，出现问题：还是有同质化的因子
改进方法：
1. 实施“相关性去重” (Correlation Filtering)
在保存因子或每一代进化时，增加一个过滤器，防止5个因子就是1个因子的情况d
2. 清洗算子集 (Operator Cleaning)
增加逻辑算子： 因为目前全是数学运算（add, min, sqrt等等）
加入：
Ts_Rank(X, days): 时序排名，量化神器。
If_Else(Condition, A, B): 引入非线性逻辑。
Signed_Power(X, 2): 保持符号的平方，比 sqrt 对动量因子更有用。
3. 特征预处理，将因子进行预处理【这步其实已经好了】



贪婪相关性去重：
1.扩大候选池：est._best_programs 里面包含的不仅仅是最后一代的 Top 10
而是整个进化过程中（每一代）出现过的最佳个体。这给了我们更多的选择。
2.强制隔离：通过 np.corrcoef 计算相关性，如果 Alpha_New 和 Alpha_Best 的相关性超过 0.7，
说明它们本质是同一个逻辑，直接丢弃，强迫算法去选那些虽然分数略低、但逻辑完全不同的因子。

3. 因子优化（这里主要以平滑处理为主，让因子更加优秀）
对因子进行3，5日平滑处理；平滑必须在截面标准化（Z-Score）之前或之后做都可以，
但建议先做 Z-Score 再平滑，然后再做一次 Z-Score，这样数据分布更稳健。

然后统一评价，
Rank IC / ICIR: 看预测能力。通常平滑后 ICIR 会显著提升。
urnover (换手率): 必须考量。如果平滑后换手率依然高得离谱，则淘汰。
相关性去除 (Correlation Filtering)
拿着你在第三步选出的“最强形态”因子，去和你的因子库（以及其他新挖出的因子）做相关性计算。

4. 因子评价
# 算出 GP_ALPHA_0 (原始), GP_ALPHA_0_EMA_5 (优化) 等所有因子的 ICIR。
# 对于每一个GP_ALPHA_0的家族，进行内部比较，选出参数最好的那个平滑版本，抛弃了不稳定的原始版。
# 最后是去重阶段：选出5个最好的因子中，进行相关性比较，保留 ICIR 更高的那个
# 你手中的 sz100_Final_Selected_Factors.csv 将是非常干净、稳定且低相关的因子集。

特别注意：如果经过第四部只挖掘出来一个因子，那么不能用XGBoost了，可以直接用backtrade回测了
但是我们通过了对SymbolicTransformer的一些参数调整，成功挖掘出3个高质量的因子；

5. 使用机器学习（XGBoost、LightGBM、RandomForest）
在使用XGBoost合成因子的建议：（看第四步出来的因子，再决定如何去进行多因子合成）
在后续的多因子合成（加权）阶段，不要直接简单平均。因为 Alpha_1 夹在中间，如果直接平均，等于变相给这一类逻辑加了双倍杠杆。
解决方案：使用 最小二乘法 (OLS) 剥离 或 最大化 IR 加权；

正交化的步骤：
以最强的 Alpha_2 为主轴。
将 Alpha_1 对 Alpha_2 做回归，取残差（即 Alpha_1 中排除掉 Alpha_2 影响后的纯净部分）。
将 Alpha_3 对 Alpha_2 和 Alpha_1 做回归，取残差。
结果：得到三个在数学上完全不相关的纯净因子。

XGBoost的滚动训练：
不做全样本训练，防止未来函数。使用“滚动窗口”模式（例如：用过去 1 年训练，预测下个月）。

模型：
XGBoost (非线性合成) + OLS (线性加权，作为基准对比)。

在第五部分遇到的问题：
——虽然 CSV 文件里看起来都是数字，但在读取或处理过程中，某些因子列被 Pandas 识别成了 object
np.isnan() 函数无法处理 object 类型的数组，所以报错。
1. 在加载数据后，立即使用 pd.to_numeric(..., errors='coerce') 强制将所有因子列转为浮点数，无法转换的脏数据变 NaN。
2. 移除 groupby.apply：改用更稳健的显式日期循环来进行正交化回归。这不仅彻底避免了 apply 里的类型黑盒问题
3. 安全回归：在进行 OLS 回归前，显式执行 .astype(float) 并严格处理 NaN 值。

——在用OLS正交化之后，合成的因子反而更垃圾了
1. 直接用IC加权或简单等权，不用XGBoost进行机器学习了！
2. 移除正交化，使用原始因子直接进行输入，并进一步降低模型的复杂度;
3. 使用更线性的合成器用 Ridge Regression (岭回归) 替代 XGBoost
4. 双轨竞赛：脚本会自动同时跑两种方法，并在最后对比两者的 IC/ICIR，让你一目了然谁更好
测试结果：
1. 简单合成方式得到的因子比XGBoost训练得到的因子更加好，XGBoost、LightGBM、神经网络等模型确实是为非线性、高交互
因为我的因子是更加线性关系，而机器学习擅长寻找非线性关系，XGBoost反而会因为强行切割而过拟合训练集里的噪音，导致测试集效果下降

进阶方法：
Ridge Regression (岭回归)，这个更加擅长寻找线性关系
结果：岭回归表现更加差劲了。。。

这说明：因子挖掘工作做得非常扎实（单因子质量高），且因子同质化程度较高。这时候，任何花哨的加权都是画蛇添足。
直接进入回测部分！



第六部分:使用backtrade进行回测
数据部分：
1. 读取行情 (sz100.csv)：这是基准[OHLCV]；读取策略这是你的 PRED_SCORE，即我们的因子分；
2. 合并 (Merge)：将两张表按date+code对齐。每条数据Datetime, Open, High, Low, Close, Volume, Score。
选股部分：
1. 每天收盘后(理解为触发next()的时候，next()自动被调用一次),获取当天所有有数据的股票及其分数
2. 根据分数，进行排名，选出top10
3. 对于选出的top10股票，进行卖出和买入操作：
——对于 在 top 10 中 且账户持有的股票：
如果没持仓 -> 买入到 9.9%。
如果有持仓但涨多了/跌多了 -> 加仓/减仓调整回 9.9%。
——对于对于 不在 Top 10 但 账户里有 的股票：
清仓（Target = 0%） -> 全部卖出。

结算部分:
一天结束后，系统计算当天的总资产，记录当天的收益率；

数据层：合并 OHLCV + Score。
时间层：2021-01-01 -> ... -> 2024-12-31。
逻辑层 (每日循环)：next() 读取今日数据。Score 排序。Top 10 确定目标仓位。发送 Buy/Sell 信号。
执行层：检查现金 (Cash).检查价格 (Price).成功 -> 更新持仓，扣手续费。失败 -> 报 Order Canceled，什么都不做
统计层：记录净值曲线。


问题暴露：
1. Order Canceled 的问题严重，这意味着策略发出了买入指令，但没买进去；
2. 回撤过高26%
3. 我们需要加入基本面因子，目前是纯量价因子；

对于Order Canceled 问题（统计层：记录净值曲线）:
因为策略通常是在次日开盘交易，都是用开盘价买入的，因为钱不够（股票价格跳空高开等情况）这容易造成拒单；

为什么生成的收益率曲线图中，是从22年8月份开始的
1. 2020年1月 ~ 2022年8月（前70%）：这段数据被用来**“训练”**GP模型。也就是说，你的 alpha 公式（比如 GP_ALPHA_0）是看着这段数据总结出来的规律。
2. 2022年8月 ~ 2024年12月（后30%）：这段数据是**“测试集”**。
因此，在最终的生成文件中，通常只包含测试集的分数，回测的时候应该只看测试集；
因此，程序自动把“训练集”期间的预测分置为了 NaN（空值）。
要搞清楚，现在的曲线（2022.8 ~ 2024.12）才是真实的实盘模拟

回测的逻辑：
我们拿训练好的因子去预测未来的股票得分: 假设我们获取2025年12月12日的股票数据，根据这些股票的OHLCV数据去

底层逻辑的梳理：
1.用GP挖掘出来的三个因子，就是代表三个公式——
GP 的工作就是像“拼积木”一样，把 Open, High, Low, Close, Volume 这些基础积木，
用 加减乘除、求平均、求最大值 等逻辑拼在一起，试图拼出一个能预测涨跌的公式。
2.用股票的原始行情(OHLCV),带入我们的因子(公式),我们的因子是平滑处理过后的，然后还要平滑处理一下
3.2中的方法，将三个因子的值全部计算出来，就可以根据这三个因子的值计算出score_simple
score_simple是三个因子的值的综合组合出来的值；
4. 我们对score_simple的高低进行排序，选取top15买入

针对于Order Canceled 的问题的解决
1. 原先的逻辑是先买后卖，我们必须先卖后买，这样就有资金了
2. A 股必须买 100 的整数倍。Backtrader 默认会尝试买 1234 股，有些 Broker 模式会拒绝非整数，或者因为四舍五入导致资金差几块钱不够。
因此，手动计算手数，向下取整到 100 股。
3. 涨跌停限制：策略按开盘价挂单，但市场上没有卖单，Backtrader 会模拟买入失败。跳过涨停股，不买。

修改方法：  06_Backtrade.py————>06_Backtrade_quiet.py
1.放弃 order_target_percent，改用 self.buy/sell
2.Explicit Logic（显式逻辑）：先 Check 现金；每发出一笔买单，就 current_cash -= cost。
3.涨停过滤 (check_limit_up)简单的逻辑：如果 Open / Prev_Close > 1.098，我们就假设它涨停买不进，直接 continue 跳过。这能避免很多无效的“追高”指令。
4.预留现金 (reserve_cash=0.05)永远不要满仓。预留 5% 的现金，既能应对印花税和佣金，也能应对第二天开盘高开导致的资金不足。


模拟盘训练、实盘训练:
1、在实盘中的隐形成本:
——滑点：比如说:回测数据中，开盘价是10.00元，以10.00元买入,但是，在实盘中,等你成交时，价格已经跳到了 10.02 元。
或者说，你想买，必须按“卖一价”买；你想卖，必须按“买一价”卖，这是天然的滑点;
2、在回测中，通常我们会在回测中设置 千分之 1 到 千分之 2 的滑点来模拟这种损耗。
3、市场冲击，对于大资金量去买微盘股来说，自己的买单会将股价抬高太多
我们是小资金，且都是选择SZ100的蓝筹股，这个影响可以忽略;
4、流动性陷阱：跌停板卖不出去；停牌情况;
5、延迟：对于高频策略影响很大；

加入滑点影响后，收益率从53%下降到了41%
这里有一个方向可以研究：如何减少滑点损耗？
——限价单 (Limit Order) 代替市价单：你可以挂在“买一”或者“买一和卖一的中间价”等待成交（我们不是高频，通常没问题）
——降低换手率 (Turnover)：当持仓股票跌出 Top 40 时才卖出，只有新股票进入 Top 20 时才买入。
——算法交易 (Algo Trading)：

*************************还有一个比较重要的优化点：可以加入风控：止损机制*********************

==================************=================
在加入了滑点模拟后，我们的策略还有41%的收益率，还是可以的；
接下来，去进入模拟盘测试，要开始上战场了














5. 使用深度学习,可以使用时序维度（GRU/LSTM、Transformer(Encoder only)/ALSTM
你的输入不再是“今天的10个因子值”，而是“过去20天的10个因子值序列”

6. 回测阶段，我用2020-2024年的数据去挖掘GP因子，然后再经过因子处理、优化等，最终喂给XGBoost，
这个训练出来的最终大脑去选择现在我用2025年1月-4月的股票数据。那么我先获取2025年1月-4月的股票数据
，然后计算出对应的因子列的值，然后直接用XGBoost训练出来的那个json模型去判断就行了
它会给我的2025年1月-4月的股票数据的基础上，再给出一个打分（pred_score)这一列
然后我设定选取pred_score最高的top10，这样就可以选股。

特别注意的是：警惕未来函数、未来数据【详情见：future.function.txt】

